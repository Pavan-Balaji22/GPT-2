{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mlflow as mlflow\n",
    "\n",
    "# Hyper Parameters\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "nvocab = 65\n",
    "bias = True\n",
    "dropout = 0.2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.Linear(n_embed,3*n_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_table = nn.Embedding(nvocab,n_embed)\n",
    "positional_embedding = nn.Embedding(block_size,n_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = embedding_table(torch.randint(65,(batch_size,block_size)))+positional_embedding(torch.arange(block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(1000 - block_size ,(batch_size,)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.matmul(c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 128, 4, 8]), torch.Size([64, 128, 32]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "c d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = c(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "k,q,v = e.split(32,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "k.view(64,4,128,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4, 128, 8])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.view(64,128,4,8).transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                        .view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        print(k.shape,q.shape,v.shape)\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = CausalSelfAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4, 128, 8]) torch.Size([64, 4, 128, 8]) torch.Size([64, 4, 128, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000e+00,  8.4630e-01,  9.1909e-01,  ..., -1.1952e+00,\n",
       "           1.0077e+00,  2.9027e-01],\n",
       "         [-7.6402e-01,  7.9218e-01,  0.0000e+00,  ..., -7.4257e-01,\n",
       "           1.6362e-01,  0.0000e+00],\n",
       "         [ 7.0273e-02,  2.1619e-01, -3.8167e-01,  ..., -4.2901e-01,\n",
       "          -0.0000e+00, -1.9627e-01],\n",
       "         ...,\n",
       "         [ 3.3033e-01, -4.8015e-02,  2.9802e-01,  ..., -1.9262e-01,\n",
       "           7.8003e-02,  1.6547e-02],\n",
       "         [ 2.6780e-01, -1.0062e-01,  1.7785e-01,  ..., -1.8446e-01,\n",
       "           1.4190e-01, -1.6145e-02],\n",
       "         [ 3.4581e-02,  0.0000e+00,  2.5374e-01,  ..., -0.0000e+00,\n",
       "           2.3404e-01,  1.4306e-02]],\n",
       "\n",
       "        [[-1.2588e-01,  1.2317e-03,  6.9245e-01,  ..., -2.9219e-02,\n",
       "           1.4338e-01,  3.1266e-01],\n",
       "         [-2.0354e-01,  2.3665e-01,  7.1682e-01,  ..., -1.0134e-01,\n",
       "           6.6756e-02, -3.0114e-01],\n",
       "         [-2.7937e-02,  1.2498e-01, -4.2154e-02,  ..., -1.0832e-01,\n",
       "           1.6853e-01,  5.1448e-01],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  6.8533e-02,  3.5630e-01,  ..., -2.7432e-01,\n",
       "           9.3630e-02, -5.5720e-03],\n",
       "         [ 0.0000e+00, -1.9797e-01,  2.1895e-01,  ..., -3.4889e-01,\n",
       "           0.0000e+00, -3.7030e-02],\n",
       "         [ 0.0000e+00,  1.4095e-01,  0.0000e+00,  ..., -2.7842e-01,\n",
       "           1.1656e-01, -0.0000e+00]],\n",
       "\n",
       "        [[-4.4191e-01,  3.0711e-01,  1.3580e+00,  ..., -9.9033e-01,\n",
       "           2.5407e-01,  6.0220e-01],\n",
       "         [-1.1732e-01,  1.5134e-01,  9.0679e-01,  ..., -2.4875e-01,\n",
       "          -0.0000e+00,  3.3977e-01],\n",
       "         [-9.3388e-02,  2.7093e-01,  2.7867e-01,  ...,  0.0000e+00,\n",
       "          -4.7415e-01, -0.0000e+00],\n",
       "         ...,\n",
       "         [ 2.6089e-01,  0.0000e+00,  3.7102e-01,  ..., -0.0000e+00,\n",
       "           1.8216e-01, -8.3676e-02],\n",
       "         [ 2.9836e-01, -8.4271e-02,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           0.0000e+00, -1.2357e-02],\n",
       "         [ 1.8594e-01,  3.3197e-02,  2.3564e-01,  ..., -0.0000e+00,\n",
       "           1.4824e-01, -0.0000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.2974e-02, -7.1873e-01,  1.9856e-01,  ...,  3.2650e-01,\n",
       "           0.0000e+00,  4.6964e-01],\n",
       "         [ 0.0000e+00, -0.0000e+00,  8.6400e-01,  ...,  2.0823e-01,\n",
       "          -5.3443e-01,  5.0227e-01],\n",
       "         [-0.0000e+00,  9.9047e-02,  4.2459e-01,  ..., -7.4927e-02,\n",
       "           0.0000e+00, -1.2468e-01],\n",
       "         ...,\n",
       "         [ 2.5135e-01, -5.9129e-02,  4.4710e-01,  ..., -0.0000e+00,\n",
       "           1.1039e-01,  6.0217e-02],\n",
       "         [ 0.0000e+00,  5.3421e-02,  2.2074e-01,  ..., -2.3003e-01,\n",
       "           5.4320e-02,  3.9793e-03],\n",
       "         [ 1.5044e-01,  0.0000e+00,  3.2922e-01,  ..., -0.0000e+00,\n",
       "           1.8910e-01, -2.5467e-02]],\n",
       "\n",
       "        [[-0.0000e+00, -2.1862e-01,  1.0886e+00,  ..., -6.3654e-01,\n",
       "           4.5007e-01, -7.2443e-01],\n",
       "         [-0.0000e+00,  2.7676e-01,  7.0317e-01,  ..., -2.3254e-01,\n",
       "           4.4248e-01, -1.3449e-01],\n",
       "         [-3.2136e-01,  8.3963e-01,  4.2577e-01,  ...,  0.0000e+00,\n",
       "          -1.5059e-01,  1.3188e-01],\n",
       "         ...,\n",
       "         [ 2.3427e-01, -0.0000e+00,  3.8599e-01,  ..., -0.0000e+00,\n",
       "           0.0000e+00,  4.0774e-02],\n",
       "         [ 1.4088e-01, -1.0398e-01,  2.2379e-01,  ..., -1.9400e-01,\n",
       "           8.0653e-02,  2.3187e-01],\n",
       "         [ 1.5405e-01,  7.9445e-02,  3.1546e-01,  ..., -3.2583e-01,\n",
       "           1.5729e-01, -3.0036e-02]],\n",
       "\n",
       "        [[ 2.1539e-01, -5.6337e-01,  4.8600e-01,  ..., -0.0000e+00,\n",
       "          -4.0971e-01,  2.6536e-01],\n",
       "         [ 5.9754e-01,  7.0930e-01,  5.4022e-01,  ...,  1.1797e-01,\n",
       "          -3.4065e-01, -0.0000e+00],\n",
       "         [ 2.0833e-01, -2.9871e-01, -7.5933e-02,  ...,  1.0704e-01,\n",
       "          -5.9979e-01,  3.1106e-01],\n",
       "         ...,\n",
       "         [ 1.7340e-01,  0.0000e+00,  4.1856e-01,  ..., -3.4296e-01,\n",
       "           1.7659e-01, -0.0000e+00],\n",
       "         [ 2.6430e-01, -1.4133e-01,  2.7800e-01,  ..., -0.0000e+00,\n",
       "           5.3363e-02,  6.8047e-02],\n",
       "         [ 0.0000e+00,  1.0851e-01,  2.8009e-01,  ..., -2.2121e-01,\n",
       "           0.0000e+00, -0.0000e+00]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
