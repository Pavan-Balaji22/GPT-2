{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mlflow as mlflow\n",
    "\n",
    "# Hyper Parameters\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "nvocab = 65\n",
    "bias = False\n",
    "dropout = 0.2\n",
    "Bias = False\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "c = nn.Linear(n_embd,3*n_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "embedding_table = nn.Embedding(nvocab,n_embd)\n",
    "positional_embedding = nn.Embedding(block_size,n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "d = embedding_table(torch.randint(65,(batch_size,block_size)))+positional_embedding(torch.arange(block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSelfAttention1(nn.Module):\n",
    "    def __init__(self):\n",
    "        torch.manual_seed(1337)\n",
    "        super().__init__()\n",
    "        \n",
    "        assert n_embd % n_head == 0\n",
    "        self.headsize = n_embd//n_head\n",
    "        self.csAttn = nn.Linear(n_embd,3 *n_embd ,bias = Bias)\n",
    "        self.mh_proj = nn.Linear(n_embd,n_embd ,bias = Bias)\n",
    "        self.register_buffer(\"tril\",torch.tril(torch.ones(block_size,block_size)).view(1,1,block_size,block_size))\n",
    "        self.attdropout = nn.Dropout(dropout)\n",
    "        self.mhdropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        torch.manual_seed(1337)\n",
    "        B,T,C = x.shape\n",
    "        q,k,v = self.csAttn(x).split(n_embd,dim=-1)\n",
    "        \n",
    "        k = k.view(B,T,n_head,C // n_head).transpose(1,2)\n",
    "        q = q.view(B,T,n_head,C // n_head).transpose(1,2)\n",
    "        v = v.view(B,T,n_head,C // n_head).transpose(1,2)\n",
    "        \n",
    "        wei = (q @ k.transpose(-2,-1)) * (1.0/math.sqrt(k.size(-1)))\n",
    "        wei = wei.masked_fill(self.tril[:,:,:T,:T]==0,float('-inf'))\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        wei = self.attdropout(wei)\n",
    "        out = wei @ v\n",
    "        out = out.transpose(1,2).contiguous().view(B,T,C)\n",
    "        out = self.mhdropout(self.mh_proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                    .view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        torch.manual_seed(1337)\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # print(k.shape,q.shape,v.shape)\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "ca = CausalSelfAttention()\n",
    "cb = CasualSelfAttention1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(d) == cb(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape[-1],d.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import  GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "config = GPT2Config.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.wte.weight : torch.Size([50257, 768])',\n",
       " 'transformer.wpe.weight : torch.Size([1024, 768])',\n",
       " 'transformer.h.0.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.0.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.0.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.0.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.0.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.0.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.0.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.0.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.0.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.0.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.0.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.0.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.1.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.1.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.1.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.1.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.1.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.1.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.1.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.1.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.1.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.1.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.1.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.1.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.2.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.2.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.2.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.2.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.2.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.2.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.2.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.2.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.2.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.2.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.2.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.2.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.3.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.3.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.3.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.3.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.3.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.3.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.3.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.3.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.3.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.3.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.3.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.3.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.4.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.4.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.4.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.4.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.4.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.4.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.4.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.4.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.4.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.4.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.4.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.4.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.5.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.5.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.5.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.5.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.5.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.5.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.5.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.5.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.5.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.5.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.5.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.5.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.6.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.6.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.6.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.6.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.6.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.6.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.6.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.6.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.6.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.6.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.6.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.6.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.7.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.7.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.7.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.7.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.7.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.7.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.7.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.7.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.7.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.7.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.7.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.7.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.8.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.8.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.8.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.8.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.8.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.8.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.8.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.8.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.8.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.8.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.8.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.8.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.9.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.9.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.9.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.9.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.9.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.9.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.9.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.9.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.9.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.9.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.9.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.9.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.10.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.10.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.10.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.10.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.10.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.10.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.10.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.10.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.10.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.10.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.10.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.10.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.11.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.11.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.11.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.11.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.11.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.11.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.11.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.11.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.11.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.11.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.11.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.11.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.ln_f.weight : torch.Size([768])',\n",
       " 'transformer.ln_f.bias : torch.Size([768])',\n",
       " 'lm_head.weight : torch.Size([50257, 768])']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ f\"{k} : {v.shape}\" for k,v in model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.zeros((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:,-block_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
