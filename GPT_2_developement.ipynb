{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mlflow as mlflow\n",
    "import tiktoken\n",
    "\n",
    "# Hyper Parameters\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "nvocab = 65\n",
    "bias = False\n",
    "dropout = 0.2\n",
    "Bias = False\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "c = nn.Linear(n_embd,3*n_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "embedding_table = nn.Embedding(nvocab,n_embd)\n",
    "positional_embedding = nn.Embedding(block_size,n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "d = embedding_table(torch.randint(65,(batch_size,block_size)))+positional_embedding(torch.arange(block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSelfAttention1(nn.Module):\n",
    "    def __init__(self):\n",
    "        torch.manual_seed(1337)\n",
    "        super().__init__()\n",
    "        \n",
    "        assert n_embd % n_head == 0\n",
    "        self.headsize = n_embd//n_head\n",
    "        self.csAttn = nn.Linear(n_embd,3 *n_embd ,bias = Bias)\n",
    "        self.mh_proj = nn.Linear(n_embd,n_embd ,bias = Bias)\n",
    "        self.register_buffer(\"tril\",torch.tril(torch.ones(block_size,block_size)).view(1,1,block_size,block_size))\n",
    "        self.attdropout = nn.Dropout(dropout)\n",
    "        self.mhdropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        torch.manual_seed(1337)\n",
    "        B,T,C = x.shape\n",
    "        q,k,v = self.csAttn(x).split(n_embd,dim=-1)\n",
    "        \n",
    "        k = k.view(B,T,n_head,C // n_head).transpose(1,2)\n",
    "        q = q.view(B,T,n_head,C // n_head).transpose(1,2)\n",
    "        v = v.view(B,T,n_head,C // n_head).transpose(1,2)\n",
    "        \n",
    "        wei = (q @ k.transpose(-2,-1)) * (1.0/math.sqrt(k.size(-1)))\n",
    "        wei = wei.masked_fill(self.tril[:,:,:T,:T]==0,float('-inf'))\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        wei = self.attdropout(wei)\n",
    "        out = wei @ v\n",
    "        out = out.transpose(1,2).contiguous().view(B,T,C)\n",
    "        out = self.mhdropout(self.mh_proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                    .view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        torch.manual_seed(1337)\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # print(k.shape,q.shape,v.shape)\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "ca = CausalSelfAttention()\n",
    "cb = CasualSelfAttention1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(d) == cb(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape[-1],d.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import  GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "config = GPT2Config.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.wte.weight : torch.Size([50257, 768])',\n",
       " 'transformer.wpe.weight : torch.Size([1024, 768])',\n",
       " 'transformer.h.0.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.0.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.0.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.0.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.0.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.0.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.0.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.0.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.0.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.0.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.0.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.0.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.1.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.1.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.1.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.1.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.1.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.1.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.1.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.1.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.1.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.1.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.1.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.1.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.2.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.2.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.2.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.2.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.2.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.2.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.2.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.2.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.2.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.2.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.2.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.2.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.3.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.3.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.3.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.3.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.3.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.3.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.3.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.3.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.3.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.3.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.3.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.3.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.4.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.4.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.4.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.4.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.4.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.4.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.4.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.4.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.4.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.4.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.4.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.4.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.5.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.5.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.5.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.5.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.5.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.5.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.5.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.5.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.5.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.5.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.5.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.5.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.6.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.6.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.6.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.6.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.6.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.6.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.6.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.6.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.6.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.6.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.6.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.6.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.7.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.7.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.7.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.7.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.7.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.7.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.7.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.7.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.7.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.7.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.7.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.7.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.8.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.8.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.8.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.8.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.8.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.8.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.8.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.8.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.8.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.8.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.8.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.8.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.9.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.9.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.9.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.9.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.9.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.9.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.9.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.9.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.9.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.9.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.9.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.9.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.10.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.10.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.10.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.10.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.10.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.10.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.10.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.10.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.10.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.10.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.10.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.10.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.11.ln_1.weight : torch.Size([768])',\n",
       " 'transformer.h.11.ln_1.bias : torch.Size([768])',\n",
       " 'transformer.h.11.attn.c_attn.weight : torch.Size([768, 2304])',\n",
       " 'transformer.h.11.attn.c_attn.bias : torch.Size([2304])',\n",
       " 'transformer.h.11.attn.c_proj.weight : torch.Size([768, 768])',\n",
       " 'transformer.h.11.attn.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.h.11.ln_2.weight : torch.Size([768])',\n",
       " 'transformer.h.11.ln_2.bias : torch.Size([768])',\n",
       " 'transformer.h.11.mlp.c_fc.weight : torch.Size([768, 3072])',\n",
       " 'transformer.h.11.mlp.c_fc.bias : torch.Size([3072])',\n",
       " 'transformer.h.11.mlp.c_proj.weight : torch.Size([3072, 768])',\n",
       " 'transformer.h.11.mlp.c_proj.bias : torch.Size([768])',\n",
       " 'transformer.ln_f.weight : torch.Size([768])',\n",
       " 'transformer.ln_f.bias : torch.Size([768])',\n",
       " 'lm_head.weight : torch.Size([50257, 768])']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ f\"{k} : {v.shape}\" for k,v in model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.zeros((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:,-block_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 314, 716, 350, 12421, 3652, 26436]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello I am Pavan balaji\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  torch.randint(10000,size=(10000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randint(1000,size=(64,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakspheredata= open(\"shakeshpere.txt\",mode=\"r\",encoding=\"utf8\").read()\n",
    "device = \"cpu\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "data = torch.tensor(tokenizer.encode(shakspheredata))\n",
    "B,T=4,32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[torch.randint(high=len(data),size=((B*T)+1,))]\n",
    "x = data[:-1].view(B,T)\n",
    "y = data[1:].view(B,T)\n",
    "x, y = x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.Embedding(65,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "c(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6720,  1986,    35,  3598,  3843,   683,   743,  6159,   329,   466,\n",
       "         27322,   588,   198,   464,   379,    13,   683,  2566,    13, 10686,\n",
       "           257,    43, 45648, 17062,   339,    11,    11,   705,   617,    25,\n",
       "           198,    26],\n",
       "        [  284, 39743, 13110,   290,   345,   284,    48,   268,   198,  1677,\n",
       "          1705,    26, 15125,     0,    25,   287,    56,  1276,   550,    11,\n",
       "          1242,   257,  3993,  7714,  8764,   262,   351, 16827,   815,   994,\n",
       "           766,   502],\n",
       "        [ 2740,    44, 24421,  3750,   716,   314,  2514,   674,  8128,   514,\n",
       "          3483, 13970,    11,     6,  6070, 20739,    11,   788,  4844,   339,\n",
       "         18522,   314,   257,    11,   198,   314,   286,   198,   198,   198,\n",
       "           407,  1336],\n",
       "        [  347,  2460,  3285,   198,    11,  1577,   616,    30,    25,   279,\n",
       "          2911,   428,  7813,   284,   584,    11,  1865,   503,  1011,   198,\n",
       "           534,    11,    11,   287,   319,    11, 15189,   198,  1544,     6,\n",
       "           495,   257]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
